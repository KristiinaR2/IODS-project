---
title: "chapter_4"
author: "Kristiina"
date: "23 11 2021"
output: html_document
---

```{r}
# access the MASS package
library(MASS)

library(tidyverse)
library(ggplot2)
library(corrplot)

getwd()

```
# Chapter 4: Clustering and classification

```{r}
date()
```


### 1.1 The data

The data *Boston* consists of 506 observations of 14 variables. Details of the variables are shown here: [https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html]. 


"Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them."


```{r}

# load the data
data("Boston")

str(Boston)
summary(Boston)

# plot matrix of the variables
pairs(Boston[,c(2:14)], pch = 20, cex=0.4, upper.panel=NULL)

# rounded correlation matrix
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualisation of the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)


```

### 2. Data standardization

"Standardize the dataset and print out summaries of the scaled data. 

How did the variables change?"

Here is the dataset standardized with the summaries. The function ´scale´ is generic function whose default method centers and/or scales the columns of a numeric matrix. In the scaling the column means are subtracted from the corresponding columns and the the difference is divided with standard deviation. The scaling makes the means of the variables in dataset boston 0. 

```{r}


# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)


```
### Creating the testdata

Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. 

Divide the dataset to train and test sets, so that 80% of the data belongs to the train set."

Here a categorical variable of the crime rate is created, with quantiles and the old crime rate variable dropped. The dataset is then divided to train (80% of the data) and test sets. 

```{r}

# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)


```



### 3.LDA

"Fit the linear discriminant analysis on the train set. 

Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. 

Draw the LDA (bi)plot. "


https://tuomonieminen.github.io/Helsinki-Open-Data-Science/#/40 

```{r}


# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```



"Save the crime categories from the test set and then remove the categorical crime variable from the test dataset."

"Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results." 

```{r}

# save the correct classes from test data
#correct_classes <- test$crime

# remove the crime variable from test data
#test <- dplyr::select(test, -crime)

#####

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)



```

### 4. K-means

Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations. Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results.

```{r}

data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

```



