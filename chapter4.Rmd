---
title: "chapter_4"
author: "Kristiina"
date: "23 11 2021"
output: html_document
---

```{r}
# access the MASS package
library(MASS)

library(tidyverse)
library(ggplot2)
library(corrplot)

getwd()

```
# Chapter 4: Clustering and classification

```{r}
date()
```


### 1.1 The data

The data *Boston* consists of 506 observations of 14 variables. Details of the variables are shown here: [https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html]. 


"Show a graphical overview of the data and show summaries of the variables in the data. Describe and interpret the outputs, commenting on the distributions of the variables and the relationships between them."


```{r}

# load the data
data("Boston")

str(Boston)
summary(Boston)

# plot matrix of the variables
pairs(Boston[,c(2:14)], pch = 20, cex=0.4, upper.panel=NULL)

# rounded correlation matrix
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualisation of the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)


```

### 2. Data standardization

"Standardize the dataset and print out summaries of the scaled data. 

How did the variables change?"

Linear discriminant analysis produces results based on the assumptions that
variables are normally distributed, the normal distributions for each class share the same covariance matrix, and the variables are continuous.Because of the assumptions, the data might need scaling before fitting the model.

Below is the dataset standardized with the summaries. The function ´scale´ is generic function whose default method centers and/or scales the columns of a numeric matrix. In the scaling the column means are subtracted from the corresponding columns and the the difference is divided with standard deviation. The scaling makes the means of the variables in dataset boston 0. 

```{r}

boston_scaled <- scale(Boston) # center and standardize variables
summary(boston_scaled)# summaries of the scaled variables
class(boston_scaled)# class of the boston_scaled object
boston_scaled <- as.data.frame(boston_scaled) # change the object to data frame


```
### Creating the testdata

Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. 

Divide the dataset to train and test sets, so that 80% of the data belongs to the train set."

Here a categorical variable of the crime rate is created, with quantiles and the old crime rate variable dropped. The dataset is then divided to train (80% of the data) and test sets. 

```{r}

summary(boston_scaled$crim) #summary of the scaled crime rate
bins <- quantile(boston_scaled$crim) #a quantile vector crim
bins
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high")) # a categorical variable 'crime'
table(crime) # table of the new factor crime

boston_scaled <- dplyr::select(boston_scaled, -crim)# original crim removed from the dataset
boston_scaled <- data.frame(boston_scaled, crime)# adding the new categorical value to scaled data

n <- nrow(boston_scaled) # number of rows in the Boston dataset 
ind <- sample(n,  size = n * 0.8) # choosing a random 80% of the rows
train <- boston_scaled[ind,] # the train set
test <- boston_scaled[-ind,] # the test set


```



### 3.LDA

https://tuomonieminen.github.io/Helsinki-Open-Data-Science/#/40 

Here a linear discriminant analysis (LDA) is fitted using the categorical crime rate as the target variable and all the other variables as predictors.

Based on the trained model LDA calculates the probabilities for the new observation for belonging in each of the classes. The observation is classified to the class of the highest probability

```{r}

lda.fit <- lda(crime ~ ., data = train) # the LDA model
lda.fit

# LDA biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime) # target classes as numeric

# plot of the LDA results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)


```

### Predicting the classes

The LDA seemed to predict the crime categories quite well, as the predictions were 74% accurate. The class with best prediction is the class ´high´, the poorest class prediction was in the class med_high.


```{r}

correct_classes <- test$crime # saving the correct classes from test data

test <- dplyr::select(test, -crime) # removing the crime variable from test data

lda.pred <- predict(lda.fit, newdata = test)# predicting the classes with test data

result <- table(correct = correct_classes, predicted = lda.pred$class) #cross tab of the results
result

diag(prop.table(result, 1)) #probability table
sum(diag(prop.table(result))) # ennustetarkkuus

```

### 4. K-means

K-means is an unsupervised method, that assigns observations to groups or clusters based on similarity of the objects. K-means needs the number of clusters as an argument.

In this exercise the the Boston dataset is first reloaded, and standardized. 

The distances between the observations are calculated: the observation belongs to the category whose center is closest to it.

The optimal number of clusters is investigated. One way to determine the number of clusters is to look at how the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes. When plotting the number of clusters and the total WCSS, the optimal number of clusters is when the total WCSS drops radically.

if I interpreted the graphs correctly, there are no clear clusters and the optimal number is 2.

```{r}

data("Boston")

boston_scaled <- scale(Boston) # center and standardize variables
class(boston_scaled) # class of the boston_scaled object
boston_scaled <- as.data.frame(boston_scaled)# changes the object to data frame

dist_eu <- dist(boston_scaled) # euclidean distance matrix
summary(dist_eu) # summary of the distances

dist_man <- dist(boston_scaled, method = 'manhattan') # manhattan distance matrix
summary(dist_man)# summary of the distances

set.seed(123) # K-means might produce different results every time, because it randomly assigns the initial cluster centers. The function set.seed deals with that.

##### here with 3 centers

km <-kmeans(boston_scaled, centers = 3) # k-means clustering

pairs(boston_scaled, col = km$cluster) # plot of the Boston dataset with clusters

library(cluster)
clusplot(boston_scaled, km$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)

####### here with 2 centers

km2 <-kmeans(boston_scaled, centers = 2) # k-means clustering

pairs(boston_scaled, col = km2$cluster) # plot of the Boston dataset with clusters

library(cluster)
clusplot(boston_scaled, km2$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)


####

k_max8 <- 8 #  the number of clusters 8

twcss8 <- sapply(1:k_max8, function(k){kmeans(boston_scaled, k)$tot.withinss}) # the total within sum of squares
twcss8

qplot(x = 1:k_max8, y = twcss8, geom = 'line') # visualizing the results

km <-kmeans(boston_scaled, centers = 2) # k-means clustering

pairs(boston_scaled, col = km$cluster) # plot of the Boston dataset with clusters

####

k_max3 <- 3 #  the number of clusters 3

twcss2 <- sapply(1:k_max3, function(k){kmeans(boston_scaled, k)$tot.withinss}) # the total within sum of squares
twcss2

qplot(x = 1:k_max3, y = twcss2, geom = 'line') # visualizing the results

km <-kmeans(boston_scaled, centers = 2) # k-means clustering

pairs(boston_scaled, col = km$cluster) # plot of the Boston dataset with clusters


```




