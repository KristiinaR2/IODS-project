---
output:
  html_document: default
---
# Regression and model validation


```{r}
date()

```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(GGally)
library(ggplot2)

```


## 1 Reading the data

The data consists of 166 observations (rows) of 8 variables (columns). The variables are: X (rownumber), Gender (chr: "F", "M"), Age (int), Attitude (int), deep (dbl), stra (dbl), surf (dbl), Points (int). I mutated the variable gender as a factor, and dropped the X column (the row numbers). 

```{r}

setwd("C:/LocalData/krraiha/Opiskelu HY/Jatko-opinnot/IODS-project/Data")

learning2014_2 <- read.csv("learning2014.csv")

str(learning2014_2)
head(learning2014_2, 10)
glimpse(learning2014_2)
str(learning2014_2)

#Mutated the gender as a factor. 
learning2014_3 <- learning2014_2%>%
  mutate(gender = as.factor(gender))

#deleted the X variable (rownumber)
learning2014_3 <- select(learning2014_3, -X)

str(learning2014_3)

```

## 2 Graphical overview of the data

First I explored the data with couple of scatter matrix. What I noticed was that the distributions of age (mean 25) are skewed to the right, as most of the subjects are close to 20 years. The rest of variables are quite normally distributed taking into account the sample size. The gender variable has more observation in F(n=110), than in M(n=56), that might affect the distributions. There are no too high correlations between the variables (collinearity) excluding the the variables surf and stra, that seem to measure same thing, as their correlation is 1? 


```{r}

scattermatrix1 <- pairs(learning2014_3[-1], col = learning2014_3$gender)

#just for comparison without the upper panel
scattermatrix2 <- pairs(learning2014_3[,c(2:7)], col = learning2014_3$gender, pch = 20, cex=0.4, upper.panel=NULL)
scattermatrix2

matrix3 <- ggpairs(learning2014_3, mapping = aes(col = gender, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))

gender_n <- learning2014_3%>%
  group_by(gender)%>%
  summarise(n=n())%>%
  ungroup()


```

## 3. Fitting the model

First I printed the correlation matrix again, and decided to choose the following variables to the model:

y (dependent) = Points
x (explanatory) = Attitude, deep, surf

The first model (fit) indicated that only the attitude (***) was statistically significant explanatory variable. Next I removed the deep and surf, and entered gender and stra (fit1). Still the attitude is the only one that is significant, so I will continue with the model fit2: Points (y)~Attitude (x).


```{r}

#the correlations in a separate df here as a reminder
correlations <- round(cor(learning2014_3[,c(2:7)]), 2)
correlations

fit <- lm(Points ~ Attitude + deep + surf, data = learning2014_3)
summary(fit)

#Entered the gender and stra as x. 

fit1 <- lm(Points ~ Attitude + gender + stra, data = learning2014_3)
summary(fit1)

#Left the Attitude as x. 

fit2 <- lm(Points ~ Attitude, data = learning2014_3)
summary(fit2)

qplot(Attitude, Points, data = learning2014_3) + geom_smooth(method = "lm")

```

## 4. Summary of the fitted model

The results of the model "fit2" indicate, that attitude is a statistically significant explanatory variable (F(1,164)=38.61, p<.001) for the students points. The adjusted R-squared is 0.1856, so the attitude predicts 19% of the variation of students points.  

```{r}

summary(fit2)

```

## 5. diagnostic plots

The residual plots show that the residuals of the observations are not related to the fitted values, and they occur on both sides of zero (= assumption that the size of error does not depend on explanatory variables). 

The distribution of residual terms follows more or less a normal distribution, and the Q-Q plot is almost a straight line (=assumptions of the errors being normally distributed).

Also a noticeable thing is, that there seems to be couple of outliers, that might affect the regression. 


```{r}

library(ggResidpanel)
resid_panel(fit2)

par(mfrow = c(2,2))
plot(fit2, which = c(1,2,5))

```
