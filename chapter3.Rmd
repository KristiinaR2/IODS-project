---
title: "chapter3.Rmd"
author: "Kristiina"
date: "19 11 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(GGally)
library(ggplot2)

setwd("C:/LocalData/krraiha/Opiskelu HY/Jatko-opinnot/IODS-project/Data")

```

## 2 The data

Source: [https://archive.ics.uci.edu/ml/datasets/Student+Performance#]

In the data frame 'alc' there are	370 observations of 51 variables.

I chose student's sex, the quality of their family relationships, and parents educational background to observe in relation to the use of alcohol. 

alc$high_use (logi) The variable high_use is a logical one: TRUE/FALSE. 
alc$sex (chr) student's sex (binary: 'F' - female or 'M' - male)
alc$famrel (int) quality of family relationships (numeric: from 1 - very bad to 5 - excellent)
alc$Medu (int) mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)
alc$Fedu (int)  father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)


```{r}

alc <- read.csv2("https://github.com/rsund/IODS-project/raw/master/data/alc.csv", sep = "," , header=TRUE)
str(alc)
colnames(alc)

#Mutated famrel, medu & fedu to factors
alc2 <- alc %>%
  mutate(
    famrel= factor(famrel, levels = c(1,2,3,4,5), labels = c("very bad","bad","neutral","good","very good")),
         Medu = factor(Medu, levels = c(0,1,2,3,4), labels = c("none","primary","5-9th","sec ed", "high ed")),
         Fedu = factor(Fedu, levels = c(0,1,2,3,4), labels = c("none","primary","5-9th","sec ed", "high ed")))

str(alc2)


```

## 3 The hypotheses

I chose student's sex, the quality of their family relationships, and parents educational background to observe in relation to the use of alcohol. 

My hypotheses are: 
I. The sex male predicts high alcohol use
II. The quality of family relationships predicts high alcohol use
III. Mothers educational level predicts high alcohol use
IV. Fathers educational level predicts high alcohol use

```{r}

gather(alc2) %>% glimpse
gather(alc2) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()


```


## 4 The variables


```{r}

#Alcohol use & high_use
ggplot(data = alc2, aes(x = alc_use))+ geom_bar()
ggplot(data = alc2, aes(x = high_use))+ geom_bar()

# The counts of different groups
alc2 %>% group_by(high_use) %>% summarise(count = n())
alc2 %>% group_by(high_use, sex) %>% summarise(count = n())


# The aggregate data and the data frame
alc_agg2 <- alc %>%
  group_by(high_use)%>%
  summarise(
            famrel_mean = round(mean(famrel), digits = 2),
            famrel_sd = round(sd(famrel), digits = 2),
            medu_mean = round(mean(Medu), digits = 2),
            medu_sd = round(sd(Medu), digits = 2),
            fedu_mean = round(mean(Fedu), digits = 2),
            fedu_sd = round(sd(Fedu), digits = 2),
             n=n())
alc_agg2

```

### 1. Sex 

```{r}

# The frequencies & and the relative frequencies and the bar plots of high-use/low-use in the group sex 

table(alc2$high_use, alc2$sex)

round(prop.table(table(alc2$high_use, alc2$sex),2)*100)

alc2 %>% group_by(high_use, sex) %>% summarise(count = n())%>%ungroup()

ggplot(data = alc2, aes(x = high_use))+ geom_bar() + facet_wrap(~sex)


```

### 2. Family relationships 

```{r}

# The frequencies & percentages of high-use by the different family relation points
table(alc2$high_use, alc2$famrel)
prop.table(table(alc2$high_use, alc2$famrel))*100

# Plots 
ggplot(data = alc2, aes(x = famrel))+ geom_bar()
ggplot(data = alc2, aes(x = famrel))+ geom_bar() + facet_wrap(~high_use)

alc_agg %>%
  ggplot(aes(high_use, famrel_mean, group = sex, colour = sex)) + 
  geom_line(alpha=.7) + 
  geom_point() + theme_classic()
```

### 3. Mothers education

```{r}

# The frequencies & percentages of high-use by the different mothers educational background points
table(alc2$high_use, alc2$Medu)
prop.table(table(alc2$high_use, alc2$Medu))*100

# Plots
ggplot(data = alc2, aes(x = Medu))+ geom_bar()
ggplot(data = alc2, aes(x = Medu))+ geom_bar() + facet_wrap(~high_use)

alc_agg %>%
  ggplot(aes(high_use, medu_mean, group = sex, colour = sex)) + 
  geom_line(alpha=.7) + 
  geom_point() + theme_classic()

```

### 4. Fathers education

```{r}

# The frequencies & percentages of high-use by the different fathers educational background points
table(alc2$high_use, alc2$Fedu) #xtabs(~ high_use + Fedu, data=alc) optional way
prop.table(table(alc2$high_use, alc2$Fedu))*100

#Plots 
ggplot(data = alc2, aes(x = Fedu))+ geom_bar()
ggplot(data = alc2, aes(x = Fedu))+ geom_bar() + facet_wrap(~high_use)


alc_agg %>%
  ggplot(aes(high_use, fedu_mean, group = sex, colour = sex)) + 
  geom_line(alpha=.7) + 
  geom_point() + theme_classic()



```

### Summary of the data and the chosen variables

The frequency of observations in the high-use category is lower than in the low-use (high use n=111, low-use n=259). The sexes are almost equally distributed, (f=195, m=175). 

The distribution of the family relationship points shows that the values 4 and 5 have the most observations, and the family relations are bad in minority of the cases (n (1-2)= 36, n (3-5) = 344). The distribution of parents educational level shows that the educational level of mothers in a bit higher that the fathers. 

The frequency of high usage is higher with the male group (males: high-use n= 70, low-use n=105), than females (females: high-use n= 41, low-use n=154).

The means of the family relations and mothers’ education are higher in the low-usage group. The mean of fathers’ education is slightly higher in the high-use group. The difference is in the high-use female group: the mean of the fathers education is higher in the high-use group, as for the males in high-use group the mean of the fathers education is lower.

As for the hypotheses I set earlier, according to the means and distributions of the frequencies, the male sex seems to be associated with a higher frequency of alcohol use (I), as does the poorer quality of family relationships (II) and mothers lower educational level (III), but the fathers educational level is not associated with the lower alcohol consumption in females (IV).


## 5 The model

In the first model m = (high_use ~ sex + famrel + Medu + Fedu) only the sex was statistically significant. (Here, the sex male = 1, as the sex female = 0) 

The deviance residuals of the model are quite close to 0, and roughly symmetrical, but lean on the max values. After excluding the famrel, medu and fedu from the model (m2), the AIC (435.26) is still almost the same as it was in the previous model (438.88).

In the second fitted model m2 (high_use ~ sex, data = alc2) the coefficients show, that the estimate for male high alcohol being use is 0.9179 (p<.000), that indicates the odds of a males high alcohol consumption over the odds of a females high alcohol consumption.

High-use log(odds) male = -1.3234 + 0.9179 * 1 = -0.4055
High-use log(odds) female = -1.3234 + 0.9179 * 0 = -1.3234

The odds ratio (OR) is 2,5 (CI[1,59 - 3,98]), that indicates there is a positive association with the sex (male) and high alcohol consumption. 

"the exponents of the coefficients of a logistic regression model can be interpret as odds ratios between a unit change (vs no change) in the corresponding explanatory variable."

My hypotheses were: 

I. The sex male predicts high alcohol use
II. The quality of family relationships predicts high alcohol use
III. Mothers educational level predicts high alcohol use
IV. Fathers educational level predicts high alcohol use

According to the logistic regression model the sex of the subjects has a statistical relationship with high/low alcohol consumption: the odds of the male of high alcohol consumption are bigger than in females. The first hypothesis was confirmed, the others can be refuted.


```{r}

m <- glm(high_use ~ sex + famrel + Medu + Fedu, data = alc2, family = "binomial")
summary(m)

m2 <- glm(high_use ~ sex, data = alc2, family = "binomial")
summary(m2)

# odds ratios (OR) and confidence intervals (CI)
OR <- coef(m2) %>% exp
OR # = 2,5 (sexM)

CI <- confint(m2) %>% exp
CI # 1,59 - 3,98

cbind(OR, CI)


```


## 6 The predictive power of the model

The average number of wrong predictions in the cross validation is 0.31. 

```{r}

# Predict() the probability of high_use
probabilities <- predict(m2, type = "response")

# Add the predicted probabilities to 'alc'
alc2 <- mutate(alc2, probability = probabilities)

# The probabilities to make a prediction of high_use
alc2 <- mutate(alc2, prediction = probability > 0.5)

# The target variable versus the predictions
table(high_use = alc2$high_use, prediction = alc2$prediction)

# A plot of 'high_use' versus 'probability' in 'alc', define the geom as points and draw the plot
ggplot(alc2, aes(x = probability, y = high_use, col = prediction))+ geom_point()

# tabulate the target variable versus the predictions
table(high_use = alc2$high_use, prediction = alc2$prediction) %>% prop.table %>% addmargins

# define a loss function (average prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5
  mean(n_wrong)
}

# call loss_func to compute the average number of wrong predictions in the (training) data
loss_func(class = alc2$high_use, prob = alc2$probability)

# K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc2, cost = loss_func, glmfit = m, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]

######
#I found an applied this demo script from here:  https://github.com/StatQuest/logistic_regression_demo/blob/master/logistic_regression_demo.R

predicted.data <- data.frame(
  probability.of.high.a = m2$fitted.values,
  sex=alc2$sex)

ggplot(data=predicted.data, aes(x=sex, y=probability.of.high.a)) +
  geom_point(aes(color=sex), size=5) +
  xlab("Sex") +
  ylab("Predicted probability of high alcohol consumption")


######

```

## 7

Bonus: Perform 10-fold cross-validation on your model. Does your model have better test set performance (smaller prediction error using 10-fold cross-validation) compared to the model introduced in DataCamp (which had about 0.26 error). Could you find such a model? (0-2 points to compensate any loss of points from the above exercises)


## 8

Super-Bonus: Perform cross-validation to compare the performance of different logistic regression models (= different sets of predictors). Start with a very high number of predictors and explore the changes in the training and testing errors as you move to models with less predictors. Draw a graph displaying the trends of both training and testing errors by the number of predictors in the model. (0-4 points to compensate any loss of points from the above exercises)



